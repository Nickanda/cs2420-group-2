# -*- coding: utf-8 -*-
"""Copy of PostInterim_MoIST_120224.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TcJ69xuojU0T5oH-la7XAvabBLu3jB3F
"""

!pip install fvcore
import torch
from torch import nn
from torch.utils.data import DataLoader
import torch.nn.functional as F
from torchvision import datasets, transforms
from transformers import AdamW
from torchvision.models import resnet18
from fvcore.nn import FlopCountAnalysis
import matplotlib.pyplot as plt
import numpy as np
import time
from collections import defaultdict

class Config:
    in_channels = 3
    num_classes = 10
    batch_size = 64
    lr = 1e-3
    epochs = 15
    num_students = 6
    hidden_dim = 256
    temperature = 3.0
    alpha = 0.7
    teacher_model_path = "teacher.pth"
    student_model_path = "student_{}.pth"

config = Config()

def get_data_loaders():
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)
    test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)

    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2)
    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)

    return train_loader, test_loader


class TeacherModel(nn.Module):
    def __init__(self, num_classes=config.num_classes):
        super(TeacherModel, self).__init__()
        self.network = resnet18(pretrained=True)
        self.network.fc = nn.Linear(self.network.fc.in_features, num_classes)

    def forward(self, x):
        return self.network(x)

class SingleConvResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(SingleConvResidualBlock, self).__init__()
        self.conv = nn.Conv2d(
            in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False
        )
        self.bn = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample  # To match dimensions if needed

    def forward(self, x):
        identity = x  # Save input for the skip connection

        out = self.conv(x)
        out = self.bn(out)
        out = self.relu(out)

        # If dimensions mismatch, adjust the input
        if self.downsample is not None:
            identity = self.downsample(x)

        out = out + identity  # Add skip connection without modifying in place
        out = self.relu(out)

        return out
    
class StudentModel(nn.Module):
    def __init__(self, num_classes=config.num_classes):
        super(StudentModel, self).__init__()
        self.in_channels = 16  # Initial number of channels

        # Initial convolution layer
        self.conv_initial = nn.Sequential(
            nn.Conv2d(3, self.in_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(self.in_channels),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2)  # Reduces spatial dimensions by half
        )

        # Define residual layers using single-convolution residual blocks
        self.layer1 = self._make_layer(SingleConvResidualBlock, 16, blocks=2, stride=2)
        self.layer1 = self._make_layer(SingleConvResidualBlock, 32, blocks=2, stride=2)
        self.layer2 = self._make_layer(SingleConvResidualBlock, 64, blocks=2, stride=2)

        # Classification head
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(64, config.hidden_dim)
        self.relu = nn.ReLU(inplace=True)
        self.fc2 = nn.Linear(config.hidden_dim, num_classes)

    # Initialize weights
    def forward(self, x):
        x = self.conv_initial(x)

        x = self.layer1(x)
        x = self.layer2(x)

        x = self.avgpool(x)
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)

        return x

    def _make_layer(self, block, out_channels, blocks, stride=1):
        """
        Creates a layer consisting of multiple single-convolution residual blocks.

        Args:
            block: The residual block class (SingleConvResidualBlock).
            out_channels: Number of output channels for the blocks.
            blocks: Number of residual blocks in this layer.
            stride: Stride for the first block in this layer.

        Returns:
            A sequential container of residual blocks.
        """
        downsample = None
        # If there's a change in the number of channels or stride, define a downsampling layer
        if stride != 1 or self.in_channels != out_channels:
            downsample = nn.Sequential(
                nn.Conv2d(
                    self.in_channels,
                    out_channels,
                    kernel_size=1,
                    stride=stride,
                    bias=False
                ),
                nn.BatchNorm2d(out_channels),
            )

        layers = []
        # First block may need downsampling
        layers.append(block(self.in_channels, out_channels, stride, downsample))
        self.in_channels = out_channels  # Update for next blocks

        # Remaining blocks
        for _ in range(1, blocks):
            layers.append(block(out_channels, out_channels))

        return nn.Sequential(*layers)
        
def distill_teacher_to_student(teacher, student, loader, optimizer, criterion, device):
    teacher.eval()
    student.train()
    total_loss = 0
    num_correct = 0
    total_samples = 0  # Keep track of total number of samples
    
    for inputs, targets in loader:
        inputs, targets = inputs.to(device), targets.to(device)

        with torch.no_grad():
            teacher_outputs = teacher(inputs)
            teacher_soft = F.softmax(teacher_outputs / config.temperature, dim=1)

        student_outputs = student(inputs)
        student_soft = F.log_softmax(student_outputs / config.temperature, dim=1)

        distill_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (config.temperature ** 2)
        hard_loss = F.cross_entropy(student_outputs, targets)
        loss = config.alpha * distill_loss + (1 - config.alpha) * hard_loss
        
        # accumulate accuracy
        num_correct += (student_outputs.argmax(1) == targets).sum().item()
        total_samples += inputs.size(0)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    
    accuracy = num_correct / total_samples
    print(f"Distill Loss: {total_loss / len(loader):.4f}, Acc: {accuracy:.4f}")
    return total_loss / len(loader)

def evaluate_with_metrics(model, loader, device, description="Model"):
    model.eval()
    criterion = nn.CrossEntropyLoss()
    total_loss, correct = 0, 0
    total_samples = 0
    
    with torch.no_grad():        
        for i, (inputs, targets) in enumerate(loader):
            inputs, targets = inputs.to(device), targets.to(device)
            batch_size = inputs.size(0)

            if i == 0:
                start_time = time.time()

                flops_input = inputs[:1].to(device)
                flops_analysis = FlopCountAnalysis(model, flops_input)
                flops_per_image = flops_analysis.total() / batch_size

                end_time = time.time()

                latency = (end_time - start_time) / batch_size

            outputs = model(inputs)

            loss = criterion(outputs, targets)
            total_loss += loss.item()
            correct += (outputs.argmax(1) == targets).sum().item()
            total_samples += batch_size

    accuracy = correct / total_samples
    print(f"{description} Results:")
    print(f"Loss: {total_loss / len(loader):.4f}, Accuracy: {accuracy:.4f}")
    print(f"Latency per Image: {latency:.6f} secs")
    print(f"FLOPs per Image: {flops_per_image / 1e6:.2f} MFLOPs")

    return total_loss, accuracy, latency, flops_per_image

def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Get CIFAR-10 data loaders
    train_loader, test_loader = get_data_loaders()

    # Load Teacher Model
    teacher = TeacherModel().to(device)
    teacher.load_state_dict(torch.load("teacher.pth", map_location=device))
    teacher.eval()  # Ensure the teacher is in evaluation mode
    print("Teacher model loaded successfully.")

    # Distillation Phase: Teacher -> Students
    print("\nDistilling Teacher Knowledge into Students:")
    students = [StudentModel().to(device) for _ in range(config.num_students)]
    for i, student in enumerate(students):
        optimizer_student = AdamW(student.parameters(), lr=config.lr)
        for epoch in range(config.epochs):
            distill_teacher_to_student(teacher, student, train_loader, optimizer_student, nn.CrossEntropyLoss(), device)

        # Evaluate and visualize specialization
        print("\nEvaluating Student" + str(i + 1) + "'s Model:")
        evaluate_with_metrics(student, test_loader, device, description="Student")
        
        # Save the student model
        torch.save(student.state_dict(), config.student_model_path.format(i))


if __name__ == "__main__":
    main()
