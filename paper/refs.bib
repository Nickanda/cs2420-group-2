@misc{hinton2015distillingknowledgeneuralnetwork,
  title         = {Distilling the Knowledge in a Neural Network},
  author        = {Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
  year          = {2015},
  eprint        = {1503.02531},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1503.02531}
}

@inproceedings{chang-etal-2022-one,
  title     = {One-Teacher and Multiple-Student Knowledge Distillation on Sentiment Classification},
  author    = {Chang, Xiaoqin  and
               Lee, Sophia Yat Mei  and
               Zhu, Suyang  and
               Li, Shoushan  and
               Zhou, Guodong},
  editor    = {Calzolari, Nicoletta  and
               Huang, Chu-Ren  and
               Kim, Hansaem  and
               Pustejovsky, James  and
               Wanner, Leo  and
               Choi, Key-Sun  and
               Ryu, Pum-Mo  and
               Chen, Hsin-Hsi  and
               Donatelli, Lucia  and
               Ji, Heng  and
               Kurohashi, Sadao  and
               Paggio, Patrizia  and
               Xue, Nianwen  and
               Kim, Seokhwan  and
               Hahm, Younggyun  and
               He, Zhong  and
               Lee, Tony Kyungil  and
               Santus, Enrico  and
               Bond, Francis  and
               Na, Seung-Hoon},
  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
  month     = oct,
  year      = {2022},
  address   = {Gyeongju, Republic of Korea},
  publisher = {International Committee on Computational Linguistics},
  url       = {https://aclanthology.org/2022.coling-1.614},
  pages     = {7042--7052},
  abstract  = {Knowledge distillation is an effective method to transfer knowledge from a large pre-trained teacher model to a compacted student model. However, in previous studies, the distilled student models are still large and remain impractical in highly speed-sensitive systems (e.g., an IR system). In this study, we aim to distill a deep pre-trained model into an extremely compacted shallow model like CNN. Specifically, we propose a novel one-teacher and multiple-student knowledge distillation approach to distill a deep pre-trained teacher model into multiple shallow student models with ensemble learning. Moreover, we leverage large-scale unlabeled data to improve the performance of students. Empirical studies on three sentiment classification tasks demonstrate that our approach achieves better results with much fewer parameters (0.9{\%}-18{\%}) and extremely high speedup ratios (100X-1000X).}
}

@misc{fedus2022switchtransformersscalingtrillion,
  title         = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author        = {William Fedus and Barret Zoph and Noam Shazeer},
  year          = {2022},
  eprint        = {2101.03961},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2101.03961}
}