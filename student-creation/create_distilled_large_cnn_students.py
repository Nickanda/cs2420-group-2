# -*- coding: utf-8 -*-
"""Copy of PostInterim_MoIST_120224.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TcJ69xuojU0T5oH-la7XAvabBLu3jB3F
"""

!pip install fvcore
import torch
from torch import nn
from torch.utils.data import DataLoader
import torch.nn.functional as F
from torchvision import datasets, transforms
from transformers import AdamW
from torchvision.models import resnet18
from fvcore.nn import FlopCountAnalysis
import matplotlib.pyplot as plt
import numpy as np
import time
from collections import defaultdict

class Config:
    in_channels = 3
    num_classes = 10
    batch_size = 64
    lr = 1e-3
    epochs = 15
    num_students = 6
    hidden_dim = 256
    temperature = 3.0
    alpha = 0.7
    teacher_model_path = "teacher.pth"
    student_model_path = "student_{}.pth"

config = Config()

def get_data_loaders():
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)
    test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)

    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2)
    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)

    return train_loader, test_loader


class TeacherModel(nn.Module):
    def __init__(self, num_classes=config.num_classes):
        super(TeacherModel, self).__init__()
        self.network = resnet18(pretrained=True)
        self.network.fc = nn.Linear(self.network.fc.in_features, num_classes)

    def forward(self, x):
        return self.network(x)

class StudentModel(nn.Module):
    def __init__(self):
        super(StudentModel, self).__init__()
        self.network = nn.Sequential(
            # Original layers
            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # Output: 32 x 32 x 32
            nn.ReLU(),
            nn.MaxPool2d(2, 2),                          # Output: 16 x 16 x 32

            nn.Conv2d(32, 64, kernel_size=3, padding=1), # Output: 16 x 16 x 64
            nn.ReLU(),
            nn.MaxPool2d(2, 2),                          # Output: 8 x 8 x 64

            # First additional convolutional layer
            nn.Conv2d(64, 128, kernel_size=3, padding=1), # Output: 8 x 8 x 128
            nn.ReLU(),

            # Second additional convolutional layer
            nn.Conv2d(128, 256, kernel_size=3, padding=1), # Output: 8 x 8 x 256
            nn.ReLU(),

            # Pooling layer to reduce spatial dimensions
            nn.MaxPool2d(2, 2),                            # Output: 4 x 4 x 256

            nn.Flatten(),
            nn.Linear(4 * 4 * 256, config.hidden_dim),
            nn.ReLU(),
            nn.Linear(config.hidden_dim, config.num_classes)
        )


    def forward(self, x):
        return self.network(x)
        
def distill_teacher_to_student(teacher, student, loader, optimizer, criterion, device):
    teacher.eval()
    student.train()
    total_loss = 0
    num_correct = 0
    total_samples = 0  # Keep track of total number of samples
    
    for inputs, targets in loader:
        inputs, targets = inputs.to(device), targets.to(device)

        with torch.no_grad():
            teacher_outputs = teacher(inputs)
            teacher_soft = F.softmax(teacher_outputs / config.temperature, dim=1)

        student_outputs = student(inputs)
        student_soft = F.log_softmax(student_outputs / config.temperature, dim=1)

        distill_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (config.temperature ** 2)
        hard_loss = F.cross_entropy(student_outputs, targets)
        loss = config.alpha * distill_loss + (1 - config.alpha) * hard_loss
        
        # accumulate accuracy
        num_correct += (student_outputs.argmax(1) == targets).sum().item()
        total_samples += inputs.size(0)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    
    accuracy = num_correct / total_samples
    print(f"Distill Loss: {total_loss / len(loader):.4f}, Acc: {accuracy:.4f}")
    return total_loss / len(loader)

def evaluate_with_metrics(model, loader, device, description="Model"):
    model.eval()
    criterion = nn.CrossEntropyLoss()
    total_loss, correct = 0, 0
    total_samples = 0
    
    with torch.no_grad():        
        for i, (inputs, targets) in enumerate(loader):
            inputs, targets = inputs.to(device), targets.to(device)
            batch_size = inputs.size(0)

            if i == 0:
                start_time = time.time()

                flops_input = inputs[:1].to(device)
                flops_analysis = FlopCountAnalysis(model, flops_input)
                flops_per_image = flops_analysis.total() / batch_size

                end_time = time.time()

                latency = (end_time - start_time) / batch_size

            outputs = model(inputs)

            loss = criterion(outputs, targets)
            total_loss += loss.item()
            correct += (outputs.argmax(1) == targets).sum().item()
            total_samples += batch_size

    accuracy = correct / total_samples
    print(f"{description} Results:")
    print(f"Loss: {total_loss / len(loader):.4f}, Accuracy: {accuracy:.4f}")
    print(f"Latency per Image: {latency:.6f} secs")
    print(f"FLOPs per Image: {flops_per_image / 1e6:.2f} MFLOPs")

    return total_loss, accuracy, latency, flops_per_image

def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Get CIFAR-10 data loaders
    train_loader, test_loader = get_data_loaders()

    # Load Teacher Model
    teacher = TeacherModel().to(device)
    teacher.load_state_dict(torch.load("teacher.pth", map_location=device))
    teacher.eval()  # Ensure the teacher is in evaluation mode
    print("Teacher model loaded successfully.")

    # Distillation Phase: Teacher -> Students
    print("\nDistilling Teacher Knowledge into Students:")
    students = [StudentModel().to(device) for _ in range(config.num_students)]
    for i, student in enumerate(students):
        optimizer_student = AdamW(student.parameters(), lr=config.lr)
        for epoch in range(config.epochs):
            distill_teacher_to_student(teacher, student, train_loader, optimizer_student, nn.CrossEntropyLoss(), device)

        # Evaluate and visualize specialization
        print("\nEvaluating Student" + str(i + 1) + "'s Model:")
        evaluate_with_metrics(student, test_loader, device, description="Student")
        
        # Save the student model
        torch.save(student.state_dict(), config.student_model_path.format(i))


if __name__ == "__main__":
    main()
